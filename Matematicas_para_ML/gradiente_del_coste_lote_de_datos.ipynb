{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Gradiente del coste de un lote de datos"
      ],
      "metadata": {
        "id": "pjm6gUqeV3bD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "En este cuaderno, ampliamos el cálculo de la derivada parcial del notebook [*Gradiente de Regresión en un Punto*](https://github.com/joanby/matematicas-ml/blob/master/notebooks/single-point-regression-gradient.ipynb) para:\n",
        "\n",
        "* Calcular el gradiente del error cuadrático medio en un lote de datos\n",
        "* Visualizar el descenso de gradiente en acción"
      ],
      "metadata": {
        "id": "cAGScxdRV5Sa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "3MtS0SFrV-aL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "xs=torch.tensor([0,1,2,3,4,5,6,7.])\n",
        "ys=torch.tensor([1.86,1.31,.62,.33,.09,-.67,-1.23,-1.37])"
      ],
      "metadata": {
        "id": "hx9889mRWnuv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def regression(my_x,my_m,my_b):\n",
        "  return my_m*my_x+my_b"
      ],
      "metadata": {
        "id": "5-sHPJvjW-x5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "m=torch.tensor(0.9,requires_grad=True)\n",
        "b=torch.tensor(0.1,requires_grad=True)"
      ],
      "metadata": {
        "id": "hjm3Pt1hYKRn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Paso 1**: Paso hacia adelante"
      ],
      "metadata": {
        "id": "oTVcCvn8YW-g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "yhats=regression(xs,m,b)\n",
        "yhats"
      ],
      "metadata": {
        "id": "omW2hw2YYYFd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Paso 2**: Comparamos $\\hat{y}$ con el valor real de $y$ para caluclar el coste $C$"
      ],
      "metadata": {
        "id": "u2UwpSPCYgl2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Como en el notebook [*Regresión en PyTorch*](https://github.com/joanby/matematicas-ml/blob/master/notebooks/regression-in-pytorch.ipynb), vamos a utilizar el error cuadrático medio, que promedia el coste cuadrático a través de múltiples puntos de datos: $$C = \\frac{1}{n} \\sum_{i=1}^n (\\hat{y_i}-y_i)^2 $$"
      ],
      "metadata": {
        "id": "GQK8RUK5Ynd2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def mse(my_yhat,my_y):\n",
        "  sigma=torch.sum((my_yhat-my_y)**2)\n",
        "  return sigma/len(my_y)"
      ],
      "metadata": {
        "id": "Qt4jy08jYtT8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "C=mse(yhats,ys)\n",
        "C"
      ],
      "metadata": {
        "id": "KqVTG4s_ZJbF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Paso 3**: Utilizar autodiff para calcular el gradiente de $C$ en función de los parámetros"
      ],
      "metadata": {
        "id": "EDE5z2mrbstI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "C.backward()"
      ],
      "metadata": {
        "id": "R5ni6NB6buVv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "m.grad"
      ],
      "metadata": {
        "id": "IbIgQEZpbySA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "b.grad"
      ],
      "metadata": {
        "id": "XgXjT7Okb1mo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Volvemos a *Cálculo II* desliza aquí para derivar $\\frac{\\partial C}{\\partial m}$ y $\\frac{\\partial C}{\\partial b}$.**"
      ],
      "metadata": {
        "id": "b2yYG1FmkTYC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$$ \\frac{\\partial C}{\\partial m} = \\frac{2}{n} \\sum (\\hat{y}_i - y_i) \\cdot x_i $$"
      ],
      "metadata": {
        "id": "UMCQzRsnlDff"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "2*1/len(ys)*torch.sum((yhats-ys)*xs)"
      ],
      "metadata": {
        "id": "AtTXbhvjlIW4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "2*1/len(ys)*torch.sum(yhats-ys)"
      ],
      "metadata": {
        "id": "obSZ1ODbl66T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "No necesitamos crear explícitamente un objeto $\\nabla C$ independiente (delta griego invertido se llama *nabla* por «arpa» pero con gradiente es *del* como en «del C») para que el resto del código de este cuaderno se ejecute, pero vamos a crearlo por diversión ahora de todos modos y haremos uso de él en un cuaderno posterior relacionado:"
      ],
      "metadata": {
        "id": "RdMC7uLQ_by2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gradient=torch.tensor([[b.grad,m.grad]]).T\n",
        "gradient"
      ],
      "metadata": {
        "id": "XbyQwcAq_4FM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def labeled_regresion_plot(my_x,my_y,my_m,my_b,my_C,include_grad=True):\n",
        "  title='Coste={}'.format('%3g' % my_C.item())\n",
        "  if include_grad:\n",
        "    xlabel='m={}, m grad={}'.format('%3g' % my_m.item(),'%3g' % my_m.grad.item())\n",
        "    ylabel='b={}, b grad={}'.format('%3g' % my_b.item(),'%3g' % my_b.grad.item())\n",
        "  else:\n",
        "    xlabel='m={}'.format('%3g' % my_m.item())\n",
        "    ylabel='b={}'.format('%3g' % my_b.item())\n",
        "\n",
        "  fig,ax=plt.subplots()\n",
        "  plt.title(title)\n",
        "  plt.xlabel(xlabel)\n",
        "  plt.ylabel(ylabel)\n",
        "\n",
        "  ax.scatter(my_x,my_y,zorder=3)\n",
        "\n",
        "  x_min,x_max=ax.get_xlim()\n",
        "  y_min=regression(x_min,my_m,my_b).detach().item()\n",
        "  y_max=regression(x_max,my_m,my_b).detach().item()\n",
        "\n",
        "  ax.set_xlim([x_min,x_max])\n",
        "  ax.set_ylim([y_min,y_max])\n",
        "  plt.grid(color='#dadada',linestyle='-')\n",
        "  _=ax.plot([x_min,x_max],[y_min,y_max],c='C1')\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ld_TbXrWAWRd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labeled_regresion_plot(xs,ys,m,b,C)"
      ],
      "metadata": {
        "id": "A76ao1cGEF4R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Paso 4**: Gradiente descendente"
      ],
      "metadata": {
        "id": "O1LPoXmTIHo1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$\\frac{\\partial C}{\\partial m} = 36,3$ indica que un aumento de $m$ corresponde a un gran aumento de $C$.\n",
        "\n",
        "Mientras tanto, $\\frac{\\partial C}{\\partial b} = 6,26$ indica que un aumento de $b$ también corresponde a un aumento de $C$, aunque mucho menor que $m$.\n",
        "\n",
        "Por lo tanto, en la primera ronda de entrenamiento, la fruta que cuelga más baja con respecto a la reducción del coste $C$ es disminuir la pendiente de la línea de regresión, $m$. También se producirá una disminución relativamente pequeña en la intersección $y$ de la línea, $b$."
      ],
      "metadata": {
        "id": "6sYdaWZWIJ_e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer=torch.optim.SGD([m,b],lr=0.01)"
      ],
      "metadata": {
        "id": "Yu9rFJZ-IO09"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer.step()"
      ],
      "metadata": {
        "id": "l9z9kpBWcjK6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "C=mse(regression(xs,m,b),ys)\n",
        "C"
      ],
      "metadata": {
        "id": "P4vSQbwhcpWC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labeled_regresion_plot(xs,ys,m,b,C,include_grad=False)"
      ],
      "metadata": {
        "id": "3pPdUPxxc3Yh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Aclarar y repetir"
      ],
      "metadata": {
        "id": "-CTyRX_bdJt7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vamos a ver más rondas de entrenamiento:"
      ],
      "metadata": {
        "id": "Ef4zu_pQdOK3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epochs=8\n",
        "for epoch in range(epochs):\n",
        "  optimizer.zero_grad() #reseteamos gradientes aceropara que no se acumilen\n",
        "  yhats=regression(xs,m,b) #paso 1\n",
        "  C=mse(yhats,ys) # paso2\n",
        "  C.backward() #paso 3\n",
        "  labeled_regresion_plot(xs,ys,m,b,C)\n",
        "  optimizer.step() #paso 4\n",
        "\n"
      ],
      "metadata": {
        "id": "a1PHMPA9dWtJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "En rondas posteriores de entrenamiento, después de que la pendiente $m$ del modelo se haya acercado a la pendiente representada por los datos, $\\frac{\\partial C}{\\partial b}$ se vuelve negativa, lo que indica una relación inversa entre $b$ y $C$. Mientras tanto, $\\frac{\\partial C}{\\partial m}$ sigue siendo positivo.\n",
        "\n",
        "Esta combinación dirige el descenso de gradiente para ajustar simultáneamente la intercepción $y$ $b$ hacia arriba y la pendiente $m$ hacia abajo con el fin de reducir el coste $C$ y, en última instancia, ajustar la línea de regresión perfectamente a los datos."
      ],
      "metadata": {
        "id": "0OtdBOFPiBvN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "LAsszLXYiLxs"
      }
    }
  ]
}