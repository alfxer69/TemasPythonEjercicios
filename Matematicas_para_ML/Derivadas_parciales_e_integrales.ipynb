{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Cálculo II: Derivadas parciales e integrales"
      ],
      "metadata": {
        "id": "-5F7FUhKhoVx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Esta clase, *Cálculo II: Derivadas Parciales e Integrales*, se basa en el cálculo de derivadas de una sola variable para introducir los gradientes y el cálculo integral. Los gradientes de aprendizaje, facilitados por el cálculo de derivadas parciales, son la base del entrenamiento con datos de la mayoría de los algoritmos de aprendizaje automático, es decir, el descenso de gradiente estocástico (SGD). Combinado con el principio de la regla de la cadena (también tratado en esta clase), el SGD permite al algoritmo de retropropagación entrenar redes neuronales profundas.\n",
        "\n",
        "El cálculo integral, por su parte, resulta útil para innumerables tareas relacionadas con el aprendizaje automático, como encontrar el área bajo la denominada «curva ROC», una métrica predominante para evaluar modelos de clasificación. El contenido de esta clase es fundamental para otras clases de la serie *Fundamentos del aprendizaje automático*, especialmente *Probabilidad y teoría de la información* y *Optimización*."
      ],
      "metadata": {
        "id": "dfeieBTZhpu4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "En el transcurso del estudio de este tema, usted:\n",
        "\n",
        "* Desarrollar una comprensión de lo que está pasando bajo el capó de los algoritmos de aprendizaje automático, incluidos los utilizados para el aprendizaje profundo.\n",
        "* Ser capaz de comprender los detalles de la derivada parcial, cálculo multivariante que es común en los documentos de aprendizaje automático, así como muchos en otros temas que subyacen ML, incluyendo la teoría de la información y algoritmos de optimización.\n",
        "* Utilizar el cálculo integral para determinar el área bajo cualquier curva dada, una tarea recurrente en ML aplicada, por ejemplo, para evaluar el rendimiento del modelo mediante el cálculo de la métrica ROC AUC."
      ],
      "metadata": {
        "id": "WLKRS4ljh58h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tenga en cuenta que este cuaderno Jupyter no está diseñado para funcionar por sí solo. Es el código que acompaña a una clase o a los vídeos de la serie [Machine Learning Foundations](https://github.com/jonkrohn/ML-foundations) de Jon Krohn, que ofrecen detalles sobre lo siguiente:**\n",
        "\n",
        "*Segmento 1: Repaso de Cálculo Introductorio*.\n",
        "\n",
        "* El método Delta\n",
        "* Diferenciación con reglas\n",
        "* AutoDiff: Diferenciación Automática\n",
        "\n",
        "\n",
        "*Segmento 2: Gradientes de Aprendizaje Automático*\n",
        "\n",
        "* Derivadas Parciales de Funciones Multivariantes\n",
        "* La regla de la cadena de derivadas parciales\n",
        "* Coste Cuadrático\n",
        "* Gradientes\n",
        "* Gradient Descent\n",
        "* Backpropagation\n",
        "* Derivadas parciales de orden superior\n",
        "\n",
        "\n",
        "*Segmento 3: Integrales*\n",
        "\n",
        "* Clasificación binaria\n",
        "* La matriz de confusión\n",
        "* La curva ROC (Receiver-Operating Characteristic)\n",
        "* Cálculo manual de integrales\n",
        "* Integración numérica con Python\n",
        "* Hallar el área bajo la curva ROC\n",
        "* Recursos para profundizar en el cálculo"
      ],
      "metadata": {
        "id": "ZniPtb2aiD-f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Segmento 1: Repaso de Cálculo Introductorio"
      ],
      "metadata": {
        "id": "I0uRiiOWiJtu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Revisar el notebook de [*Regresión en Pytorch* ](https://github.com/joanby/matematicas-ml/blob/master/notebooks/regression-in-pytorch.ipynb)."
      ],
      "metadata": {
        "id": "eDdXBVl4iWE2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Segmento 2: Gradientes aplicados al Machine Learning"
      ],
      "metadata": {
        "id": "RIfLUvbjibSm"
      }
    }
  ]
}